{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Create the environment\n",
    "class GridWorld:\n",
    "    def __init__(self, width, height, bomb_positions, gold_positions, wall_positions):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.bomb_positions = bomb_positions\n",
    "        self.gold_positions = gold_positions\n",
    "        self.wall_positions = wall_positions\n",
    "        \n",
    "    def is_valid(self, x, y):\n",
    "        if (x, y) not in self.wall_positions and 0 <= x < self.width and 0 <= y < self.height:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def step(self, position, action):\n",
    "        x, y = position\n",
    "        if action == 0:  # Up\n",
    "            y -= 1\n",
    "        elif action == 1:  # Down\n",
    "            y += 1\n",
    "        elif action == 2:  # Left\n",
    "            x -= 1\n",
    "        elif action == 3:  # Right\n",
    "            x += 1\n",
    "\n",
    "        if not self.is_valid(x, y):\n",
    "            x, y = position\n",
    "\n",
    "        reward = -1\n",
    "        if position == (x, y):  # Penalize staying in one place\n",
    "            reward = -5\n",
    "        done = False\n",
    "        if (x, y) in self.bomb_positions:\n",
    "            reward = -100\n",
    "            done = True\n",
    "        elif (x, y) in self.gold_positions:\n",
    "            reward = 100\n",
    "            done = True\n",
    "\n",
    "        return (x, y), reward, done\n",
    "\n",
    "    def print_grid(self, position):\n",
    "        for y in range(self.height):\n",
    "            row = []\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == position:\n",
    "                    row.append('A')\n",
    "                elif (x, y) in self.bomb_positions:\n",
    "                    row.append('B')\n",
    "                elif (x, y) in self.gold_positions:\n",
    "                    row.append('G')\n",
    "                elif (x, y) in self.wall_positions:\n",
    "                    row.append('|')\n",
    "                else:\n",
    "                    row.append('.')\n",
    "            print(''.join(row))\n",
    "        print()\n",
    "\n",
    "\n",
    "# Initialize the Q-table\n",
    "def init_q_table(width, height, num_actions):\n",
    "    return np.zeros((width, height, num_actions))\n",
    "\n",
    "def update_q_table(q_table, learning_rate, discount_factor, state, action, new_state, reward):\n",
    "    x, y = state\n",
    "    new_x, new_y = new_state\n",
    "    current_q = q_table[x, y, action]\n",
    "    new_q = reward + discount_factor * np.max(q_table[new_x, new_y])\n",
    "    q_table[x, y, action] += learning_rate * (new_q - current_q)\n",
    "\n",
    "# Q-learning algorithm\n",
    "def q_learning(env, q_table, num_episodes, learning_rate, discount_factor, exploration_rate, exploration_decay, max_steps_per_episode, start_position):\n",
    "    env.print_grid(start_position)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        x, y = start_position\n",
    "        done = False\n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = np.argmax(q_table[x, y])\n",
    "            if np.random.rand() < exploration_rate:\n",
    "                action = np.random.randint(4)\n",
    "\n",
    "            new_position, reward, done = env.step((x, y), action)\n",
    "            new_x, new_y = new_position\n",
    "\n",
    "            update_q_table(q_table, learning_rate, discount_factor, (x, y), action, (new_x, new_y), reward)\n",
    "            x, y = new_x, new_y\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        exploration_rate *= exploration_decay\n",
    "\n",
    "\n",
    "    print(q_table)\n",
    "    return q_table\n",
    "\n",
    "           \n",
    "# Test the trained agent\n",
    "def test_agent(env, q_table, start_position, max_steps):\n",
    "    x, y = start_position\n",
    "    path = [start_position]\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(q_table[x, y])\n",
    "        new_position, reward, done = env.step((x, y), action)\n",
    "        new_x, new_y = new_position\n",
    "\n",
    "        path.append(new_position)\n",
    "        x, y = new_x, new_y\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB..................\n",
      ".B..................\n",
      ".B..................\n",
      ".B..................\n",
      ".B..................\n",
      ".B..................\n",
      "....................\n",
      "....................\n",
      "....................\n",
      "....................\n",
      "..........B.........\n",
      "...........|........\n",
      "............B.......\n",
      "....................\n",
      "....................\n",
      "....................\n",
      "....................\n",
      "....................\n",
      "....................\n",
      "...................G\n",
      "\n",
      "[[[  32.510919     37.88981717   32.510919   -100.        ]\n",
      "  [  36.510919     39.28264361   33.88981717 -100.        ]\n",
      "  [  37.88981717   40.689539     35.28264361 -100.        ]\n",
      "  ...\n",
      "  [  -7.99924968   -7.96848766  -10.8196589    -8.21196062]\n",
      "  [  -8.06927478   -8.01061475  -10.55239576   -7.87551128]\n",
      "  [  -8.02823339  -10.49759108  -10.48605541   -8.21647166]]\n",
      "\n",
      " [[   0.            0.            0.            0.        ]\n",
      "  [   0.            0.            0.            0.        ]\n",
      "  [   0.            0.            0.            0.        ]\n",
      "  ...\n",
      "  [  -7.83306494   -7.94442417   -7.92423422   -7.93654756]\n",
      "  [  -7.76620251   -8.1836935    -7.91365731   -7.57254877]\n",
      "  [  -8.07280216   -9.37094511   -8.35380416   -7.83120394]]\n",
      "\n",
      " [[ -11.0876771   -10.39453525  -94.235199    -10.44336721]\n",
      "  [ -10.55118032  -10.63790032  -99.66767069  -10.46543617]\n",
      "  [ -10.66552964  -10.83161217  -99.52524385  -10.65242855]\n",
      "  ...\n",
      "  [  -7.40156292   -7.61441234   -7.79908283   -7.55098097]\n",
      "  [  -7.44856368   -7.58495161   -7.60700416   -7.42829174]\n",
      "  [  -7.58377784  -10.12602905   -7.79201453   -7.6384815 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ -10.93928358   -6.96177247   -7.23764129   -7.10580002]\n",
      "  [  -6.94458417   -7.08586956   -6.94290312   -7.21734349]\n",
      "  [  -7.00544217   -7.14513982   -6.85571079   -6.78459308]\n",
      "  ...\n",
      "  [  90.19800998   94.0598       90.19800998   94.0598    ]\n",
      "  [  92.119202     96.02         92.119202     96.02      ]\n",
      "  [  94.05976168   92.01876893   94.05979449   98.        ]]\n",
      "\n",
      " [[  -8.28760016   -7.18053673   -7.16388598   -7.23639754]\n",
      "  [  -7.19838407   -7.34308619   -7.01410196   -6.96116592]\n",
      "  [  -7.25194107   -7.11034084   -6.95002017   -6.92946367]\n",
      "  ...\n",
      "  [  74.06427118   96.02         91.75688225   95.24900301]\n",
      "  [  94.0598       98.           94.0598       98.        ]\n",
      "  [  96.02         94.           96.02        100.        ]]\n",
      "\n",
      " [[  -8.50071443   -7.03369793   -7.25512382   -8.90909599]\n",
      "  [  -7.107525     -6.7956533    -6.93310213   -9.47292188]\n",
      "  [  -6.7136231    -6.74115532   -6.97866024   -9.39227046]\n",
      "  ...\n",
      "  [  12.20444708   97.95991248   68.65470085   24.09790097]\n",
      "  [  83.05262789  100.           90.52446228   93.53467038]\n",
      "  [   0.            0.            0.            0.        ]]]\n",
      "Path taken by the agent from the starting position:\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(0, 4)\n",
      "(0, 5)\n",
      "(0, 6)\n",
      "(1, 6)\n",
      "(2, 6)\n",
      "(3, 6)\n",
      "(4, 6)\n",
      "(4, 7)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(5, 9)\n",
      "(6, 9)\n",
      "(7, 9)\n",
      "(7, 10)\n",
      "(7, 11)\n",
      "(7, 12)\n",
      "(7, 13)\n",
      "(7, 14)\n",
      "(7, 15)\n",
      "(7, 16)\n",
      "(8, 16)\n",
      "(9, 16)\n",
      "(10, 16)\n",
      "(11, 16)\n",
      "(12, 16)\n",
      "(13, 16)\n",
      "(14, 16)\n",
      "(15, 16)\n",
      "(16, 16)\n",
      "(16, 17)\n",
      "(16, 18)\n",
      "(17, 18)\n",
      "(17, 19)\n",
      "(18, 19)\n",
      "(19, 19)\n"
     ]
    }
   ],
   "source": [
    "# Set up the environment and Q-learning parameters\n",
    "width, height = 20, 20\n",
    "bomb_positions = [(1,1),(1,0),(1,2),(1,3),(1,4),(1,5),(10, 10), (12, 12)]\n",
    "gold_positions = [(19, 19)]\n",
    "wall_positions = [(11, 11)]\n",
    "\n",
    "env = GridWorld(width, height, bomb_positions, gold_positions, wall_positions)\n",
    "q_table = init_q_table(width, height, 4)\n",
    "\n",
    "num_episodes = 5000\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.99\n",
    "exploration_rate = 0.1\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "exploration_rate = 1.0\n",
    "exploration_decay = 0.9995\n",
    "start_position = (0, 0)\n",
    "q_table = q_learning(env, q_table, num_episodes, learning_rate, discount_factor, exploration_rate, exploration_decay, max_steps_per_episode, start_position)\n",
    "\n",
    "# Test the trained agent\n",
    "max_test_steps = 1000\n",
    "path = test_agent(env, q_table, start_position, max_test_steps)\n",
    "\n",
    "# Print the path\n",
    "print(\"Path taken by the agent from the starting position:\")\n",
    "for position in path:\n",
    "    print(position)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
